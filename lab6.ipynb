{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-qc/lab6/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENagtZdhIR52"
      },
      "source": [
        "# Lab 6 - Quantum Convolution Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7649f5"
      },
      "source": [
        "In this lab, we will learn about Quantum Convolutional Neural Network (QCNN). We will implement a QCNN in Qiskit by modeling both the convolutional layers and pooling layers using a quantum circuit. After building a network, we will train it to differentiate horizontal and vertical lines from a pixelated image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8875c12"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Differences between a QCNN and CCNN\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d397710c"
      },
      "source": [
        "### 1.1 Classical Convolutional Neural Networks\n",
        "\n",
        "Classical Convolutional Neural Networks (CCNNs) are a subclass of artificial neural networks which have the ability to determine particular features and patterns of a given input. Because of this, they are commonly used in image recognition and audio processing.\n",
        "\n",
        "The capability of determining features is a result of the two types of layers used in a CCNN, the convolutional layer and pooling layer.\n",
        "\n",
        "An example of a CCNN can be seen in Figure 1, where a CCNN is trained to determine whether an input image either contains a cat or a dog. To do so, the input image passes through a series of alternating convolutional (C) and pooling layers (P), all of which detect patterns and associate each pattern to a cat or a dog. The fully connected layer (FC) provides us with an output which allows us to determine whether the input image was a cat or dog.\n",
        "\n",
        "The convolutional layer makes  use of a kernel, which can determine features and patterns of a particular input. An example of this is feature detection in an image, where different layers detect particular patterns in the input image. This is demonstrated in Figure 1, where the $l^{th}$ layer recognizes features and patterns along the $ij$ plane. It can then associate such features with a given output in the training process, and can use this process to train the dataset.\n",
        "\n",
        "On the other hand, a pooling layer reduces the dimensionality of the input data, reducing the computational cost and amount of learning parameters in the CCNN. A schematic of a CCNN can be seen below.\n",
        "\n",
        "<a href=\"https://ibb.co/yc2W5FCQ\"><img src=\"https://i.ibb.co/21JgP3rk/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 1. A schematic demonstration of the use of a CCNN to classify between images of a cat and dog. Here, we see the several convolutional and pooling layers being applied, all of which are decreasing in dimensionality due to the use of the pooling layers. The output of the CCNN determines whether the input image was a cat or dog.*\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 Quantum Convolutional Neural Networks\n",
        "\n",
        "Quantum Convolutional Neural Networks (QCNN) behave in a similar manner to CCNNs. First, we encode our pixelated image into a quantum circuit using a given feature map, such Qiskit's `ZFeatureMap` or `ZZFeatureMap` or others available in the circuit library.\n",
        "\n",
        "After encoding our image, we apply alternating convolutional and pooling layers, as defined in the next section. By applying these alternating layers, we reduce the dimensionality of our circuit until we are left with one qubit. We can then classify our input image by measuring the output of this one remaining qubit.\n",
        "\n",
        "*The Quantum Convolutional Layer* will consist of a series of two qubit unitary operators, which recognize and determine relationships between the qubits in our circuit. This unitary gates are defined below in the next section.\n",
        "\n",
        "For the *Quantum Pooling Layer*, we cannot do the same as is done classically to reduce the dimension, i.e. the number of qubits in our circuit. Instead, we reduce the number of qubits by performing operations upon each until a specific point and then disregard certain qubits in a specific layer. It is these layers where we stop performing operations on certain qubits that we call our 'pooling layer'. Details of the pooling layer is discussed further in the next section.\n",
        "\n",
        "In the QCNN, each layer contains parametrized circuits, meaning we alter our output result by adjusting the parameters of each layer. When training our QCNN, it is these parameters that are adjusted to reduce the loss function of our QCNN.\n",
        "\n",
        "An example of four qubit QCNN can be seen below.\n",
        "\n",
        "<a href=\"https://ibb.co/wNSvdfrV\"><img src=\"https://i.ibb.co/jvyp8SZm/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 2: Example QCNN containing four qubits. The first Convolutional Layer acts on all the qubits. This is followed by the first pooling layer, which reduces the dimensionality of the QCNN from four qubits to two qubits by disregarding the first two. The second Convolutional layer then detects features between the two qubits still in use in the QCNN, followed by another pooling layer, which reduces the dimensionality from two qubits to one, which will be our output qubit.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5c01c6"
      },
      "source": [
        "---\n",
        "\n",
        "### 1.2. Components of a QCNN\n",
        "\n",
        "---\n",
        "\n",
        "As discussed previously, a CCNN will contain both convolutional and pooling layers. Here, we define these layers for the QCNN in terms of gates applied to a `Quantum Circuit` and demonstrate an example for each layer for 4 qubits.\n",
        "\n",
        "Each of these layers will contain parameters which are tuned throughout the training process to minimize the loss function and train the QCNN to classify between horizontal and vertical lines.\n",
        "\n",
        "In theory, one could apply any parametrized circuit for both the convolutional and pooling layers of our network. For example we could use the Gellmann Matrices (which are the three dimensional generalization of the Pauli Matrices) as generators for each unitary gate acting on a pair of qubits.\n",
        "\n",
        "Here, we take a different approach and form our parametrized circuit based on the two qubit unitary. This states that every unitary matrix in $U(4)$ can be decomposed such that\n",
        "\n",
        "$$U = (A_1 \\otimes A_2) \\cdot N(\\alpha, \\beta, \\gamma) \\cdot (A_3 \\otimes A_4)$$\n",
        "\n",
        "where $A_j \\in \\text{SU}(2)$, $\\otimes$ is the tensor product, and $N(\\alpha, \\beta, \\gamma) = exp(i[\\alpha \\sigma_x\\sigma_x + \\beta \\sigma_y\\sigma_y + \\gamma \\sigma_z\\sigma_z ])$, where $\\alpha, \\beta, \\gamma$ are the parameters that we can adjust.\n",
        "\n",
        "From this, it is evident that each unitary depends on 15 parameters and implies that in order for the QCNN to be able to span the whole Hilbert space, each unitary in our QCNN must contain 15 parameters each.\n",
        "\n",
        "Tuning this large amount of parameters would be difficult and would lead to long training times. To overcome this problem, we restrict our `ansatz` to a particular subspace of the Hilbert space and define the two qubit unitary gate as $N(\\alpha, \\beta, \\gamma)$. These two qubit unitaries, can be seen below and are applied to all neighboring qubits each of the layers in the QCNN.\n",
        "\n",
        "Note that by only using $N(\\alpha, \\beta, \\gamma)$ as our two qubit unitary for the parametrized layers, we are restricting our QCNN to a particular subspace, one in which the optimal solution may not be contained in and reducing the accuracy of the QCNN. For the purpose of this tutorial, we will use this parametrized circuit to decrease the training time of our QCNN.\n",
        "\n",
        "<a href=\"https://ibb.co/9zH8DnP\"><img src=\"https://i.ibb.co/xPqCkhZ/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 3: Parametrized two qubit unitary circuit for $N(\\alpha, \\beta, \\gamma) = exp(i[\\alpha \\sigma_x\\sigma_x + \\beta \\sigma_y\\sigma_y + \\gamma \\sigma_z\\sigma_z ])$ as seen in [3], where $\\alpha =  \\frac{\\pi}{2} - 2\\theta$, $\\beta = 2\\phi - \\frac{\\pi}{2}$ and $\\gamma =  \\frac{\\pi}{2} - 2\\lambda$ as seen in the circuit. This two qubit unitary will be applied to all neighboring qubits in our feature map. *\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smknQccq1rHb"
      },
      "source": [
        "We first begin by importing the libraries and packages we will need for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oemlE64C9DAk"
      },
      "outputs": [],
      "source": [
        "# Install exactly the working versions\n",
        "!pip install --no-cache-dir \"qiskit==1.1.0\" \"qiskit-machine-learning==0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy9zq4wk9OR5"
      },
      "outputs": [],
      "source": [
        "# Check if primitives are workin, if this runs smoothly, we are good.\n",
        "from qiskit.primitives import Sampler\n",
        "print(\"Sampler is now available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s99nNSnM9QKe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.circuit.library import ZFeatureMap\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from qiskit.primitives import StatevectorEstimator as Estimator\n",
        "from qiskit_machine_learning.optimizers import COBYLA\n",
        "from qiskit_machine_learning.utils import algorithm_globals\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "algorithm_globals.random_seed = 12345\n",
        "estimator = Estimator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M0uzdRuC9pBi"
      },
      "outputs": [],
      "source": [
        "# DONT FORGET TO INSTALL pylatexenc!!\n",
        "!pip install pylatexenc\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4972aa4"
      },
      "source": [
        "### 1.2.1. Convolutional Layer\n",
        "\n",
        "First, we will define the Convolutional Layers of our QCNN. These layers are then applied to the qubits after the data has been encoded through use of the feature map.\n",
        "\n",
        "To do so we first need to determine a parametrized unitary gate, which will be used to create our convolutional and pooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "uEY-GBj79bsj",
        "outputId": "2c950b09-7757-4430-8837-50ca52c12057"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 705.35x200.667 with 1 Axes>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function that creates a two-qubit parameterized quantum circuit (unitary)\n",
        "# This circuit structure is based on a reference denoted as [3] in the original context.\n",
        "def conv_circuit(params):\n",
        "    # Initialize a 2-qubit quantum circuit\n",
        "    target = QuantumCircuit(2)\n",
        "\n",
        "    # Apply an Rz rotation of -π/2 on qubit 1 (second qubit)\n",
        "    target.rz(-np.pi / 2, 1)\n",
        "\n",
        "    # Apply a CNOT gate with control qubit 1 and target qubit 0\n",
        "    target.cx(1, 0)\n",
        "\n",
        "    # Apply an Rz rotation on qubit 0 by parameter params[0]\n",
        "    target.rz(params[0], 0)\n",
        "\n",
        "    # Apply an Ry rotation on qubit 1 by parameter params[1]\n",
        "    target.ry(params[1], 1)\n",
        "\n",
        "    # Apply a CNOT gate with control qubit 0 and target qubit 1\n",
        "    target.cx(0, 1)\n",
        "\n",
        "    # Apply an Ry rotation on qubit 1 by parameter params[2]\n",
        "    target.ry(params[2], 1)\n",
        "\n",
        "    # Apply a CNOT gate with control qubit 1 and target qubit 0\n",
        "    target.cx(1, 0)\n",
        "\n",
        "    # Apply an Rz rotation of π/2 on qubit 0\n",
        "    target.rz(np.pi / 2, 0)\n",
        "\n",
        "    # Return the constructed two-qubit unitary circuit\n",
        "    return target\n",
        "\n",
        "\n",
        "# Create a vector of three parameters named \"θ\" to be used in the circuit\n",
        "params = ParameterVector(\"θ\", length=3)\n",
        "\n",
        "# Instantiate the parameterized two-qubit circuit with the defined parameters\n",
        "circuit = conv_circuit(params)\n",
        "\n",
        "# Draw the quantum circuit using matplotlib with 'clifford' style for better visualization\n",
        "circuit.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1b7140"
      },
      "source": [
        "Now that we have defined these unitaries, it is time to create a function for the convolutional layer in our QCNN. To do so, we apply the two qubit unitary to neighboring qubits as seen in the ``conv_layer`` function below.\n",
        "\n",
        "Note that we first apply the two qubit unitary to all even pairs of qubits followed by applying to odd pairs of qubits in a circular coupling manner, i.e. the as well as neighboring qubits being coupled, the first and final qubit are also coupled through a unitary gate.\n",
        "\n",
        "Note that we add barriers into our quantum circuits for convenience when plotting, however they are not required for the actual QCNN and can be extracted from the following circuits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miouWdYs-NDD"
      },
      "outputs": [],
      "source": [
        "# Define a function to create a convolutional layer quantum circuit on `num_qubits` qubits,\n",
        "# using parameterized two-qubit unitaries defined by `conv_circuit`.\n",
        "def conv_layer(num_qubits, param_prefix):\n",
        "    # Initialize a quantum circuit with the specified number of qubits and name it\n",
        "    qc = QuantumCircuit(num_qubits, name=\"Convolutional Layer\")\n",
        "\n",
        "    # Create a list of qubit indices, e.g., [0, 1, 2, 3] for 4 qubits\n",
        "    qubits = list(range(num_qubits))\n",
        "\n",
        "    # Initialize the parameter index counter\n",
        "    param_index = 0\n",
        "\n",
        "    # Create a ParameterVector with length 3 parameters per pair of qubits (each conv unitary needs 3)\n",
        "    params = ParameterVector(param_prefix, length=num_qubits * 3)\n",
        "\n",
        "    # First loop: apply conv_circuit on pairs of qubits with even-odd indices (0-1, 2-3, ...)\n",
        "    for q1, q2 in zip(qubits[0::2], qubits[1::2]):\n",
        "        # Compose the two-qubit convolution unitary on qubits q1 and q2 using 3 parameters\n",
        "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
        "        qc.barrier()  # Add a barrier for visual separation in the circuit\n",
        "        param_index += 3  # Move parameter index forward by 3\n",
        "\n",
        "    # Second loop: apply conv_circuit on pairs with shifted indices (1-2, 3-0, wrapping around)\n",
        "    for q1, q2 in zip(qubits[1::2], qubits[2::2] + [0]):\n",
        "        # Compose convolution unitary on these pairs similarly\n",
        "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
        "        qc.barrier()  # Add a barrier for clarity\n",
        "        param_index += 3  # Advance parameter index by 3 again\n",
        "\n",
        "    # Convert the entire convolutional layer circuit into a reusable instruction\n",
        "    qc_inst = qc.to_instruction()\n",
        "\n",
        "    # Create a new circuit on the original qubits\n",
        "    qc = QuantumCircuit(num_qubits)\n",
        "\n",
        "    # Append the convolutional layer instruction to this circuit on all qubits\n",
        "    qc.append(qc_inst, qubits)\n",
        "\n",
        "    # Return the complete convolutional layer circuit\n",
        "    return qc\n",
        "\n",
        "\n",
        "# Instantiate the convolutional layer on 4 qubits with parameters prefix \"θ\"\n",
        "circuit = conv_layer(4, \"θ\")\n",
        "\n",
        "# Decompose the circuit (show underlying gates) and draw it with matplotlib in clifford style\n",
        "circuit.decompose().draw(\"mpl\", style=\"clifford\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f59677b5"
      },
      "source": [
        "### 1.2.2 Pooling Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23856709"
      },
      "source": [
        "The purpose of a pooling layer is to reduce the dimensions of our Quantum Circuit, i.e. reduce the number of qubits in our circuit, while retaining as much information as possible from previously learned data. Reducing the amount of qubits also reduces the computational cost of the overall circuit, as the number of parameters that the QCNN needs to learn decreases.\n",
        "\n",
        "However, one cannot simply decrease the amount of qubits in our quantum circuit. Because of this, we must define the pooling layer in a different manner compared with the classical approach.\n",
        "\n",
        "To 'artificially' reduce the number of qubits in our circuit, we first begin by creating pairs of the $N$ qubits in our system.\n",
        "\n",
        "After initially pairing all the qubits, we apply our generalized 2 qubit unitary to each pair, as described previously. After applying this two qubit unitary, we then ignore one qubit from each pair of qubits for the remainder of the neural network.\n",
        "\n",
        "This layer therefore has the overall effect of 'combining' the information of the two qubits into one qubit by first applying the unitary circuit, encoding information from one qubit into another, before disregarding one of qubits for the remainder of the circuit and not performing any operations or measurements on it.\n",
        "\n",
        "We note that one could also apply a dynamic circuit to reduce the dimensionality in the pooling layers. This would involve performing measurements on certain qubits in the circuit and having an intermediate classical feedback loop in our pooling layers. By applying these measurements, one would also be reducing the dimensionality of the circuit.\n",
        "\n",
        "In this tutorial, we apply the former approach, and disregard qubits in each pooling layer. Using this approach, we thus create a QCNN Pooling Layer which transforms the dimensions of our $N$ qubit Quantum Circuit to $N/2$.\n",
        "\n",
        "To do so, we first define a two qubit unitary, which transforms the two qubit system to one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIoWJtPiBeFB"
      },
      "outputs": [],
      "source": [
        "# Define a function that creates a two-qubit parameterized quantum pooling circuit.\n",
        "def pool_circuit(params):\n",
        "    # Initialize a quantum circuit with 2 qubits\n",
        "    target = QuantumCircuit(2)\n",
        "\n",
        "    # Apply an Rz rotation of -π/2 on qubit 1 (second qubit)\n",
        "    target.rz(-np.pi / 2, 1)\n",
        "\n",
        "    # Apply a CNOT gate with control qubit 1 and target qubit 0\n",
        "    target.cx(1, 0)\n",
        "\n",
        "    # Apply an Rz rotation on qubit 0 by the first parameter in params\n",
        "    target.rz(params[0], 0)\n",
        "\n",
        "    # Apply an Ry rotation on qubit 1 by the second parameter in params\n",
        "    target.ry(params[1], 1)\n",
        "\n",
        "    # Apply a CNOT gate with control qubit 0 and target qubit 1\n",
        "    target.cx(0, 1)\n",
        "\n",
        "    # Apply an Ry rotation on qubit 1 by the third parameter in params\n",
        "    target.ry(params[2], 1)\n",
        "\n",
        "    # Return the constructed two-qubit pooling circuit\n",
        "    return target\n",
        "\n",
        "\n",
        "# Create a vector of three symbolic parameters named \"θ\" to be used in the pooling circuit\n",
        "params = ParameterVector(\"θ\", length=3)\n",
        "\n",
        "# Instantiate the parameterized two-qubit pooling circuit with these parameters\n",
        "circuit = pool_circuit(params)\n",
        "\n",
        "# Draw the quantum circuit using matplotlib with 'clifford' style for clear visualization\n",
        "circuit.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b9ff63b"
      },
      "source": [
        "After applying this two qubit unitary circuit, we neglect the first qubit (q0) in future layers and only use the second qubit (q1) in our QCNN\n",
        "\n",
        "We apply this two qubit pooling layer to different pairs of qubits to create our pooling layer for N qubits. As an example we then plot it for four qubits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiTUUPSnBnQ-"
      },
      "outputs": [],
      "source": [
        "# Define a function to create a pooling layer circuit using pairs of qubits from sources and sinks.\n",
        "def pool_layer(sources, sinks, param_prefix):\n",
        "    # Calculate the total number of qubits involved in the pooling layer\n",
        "    num_qubits = len(sources) + len(sinks)\n",
        "\n",
        "    # Initialize a quantum circuit with the total number of qubits and name it\n",
        "    qc = QuantumCircuit(num_qubits, name=\"Pooling Layer\")\n",
        "\n",
        "    # Initialize parameter index counter to track parameters used for each pooling unit\n",
        "    param_index = 0\n",
        "\n",
        "    # Create a ParameterVector with 3 parameters for each pair of qubits (source-sink)\n",
        "    params = ParameterVector(param_prefix, length=(num_qubits // 2) * 3)\n",
        "\n",
        "    # Loop through pairs of source and sink qubits\n",
        "    for source, sink in zip(sources, sinks):\n",
        "        # Compose the pooling circuit (defined by pool_circuit) on each source-sink pair,\n",
        "        # using a slice of 3 parameters from the ParameterVector\n",
        "        qc = qc.compose(pool_circuit(params[param_index : (param_index + 3)]), [source, sink])\n",
        "\n",
        "        # Add a barrier to visually separate operations for clarity in the circuit diagram\n",
        "        qc.barrier()\n",
        "\n",
        "        # Increment parameter index by 3 for the next pooling unit\n",
        "        param_index += 3\n",
        "\n",
        "    # Convert the constructed pooling layer circuit into a reusable instruction\n",
        "    qc_inst = qc.to_instruction()\n",
        "\n",
        "    # Create a new circuit for the total qubits to append the instruction cleanly\n",
        "    qc = QuantumCircuit(num_qubits)\n",
        "\n",
        "    # Append the pooling layer instruction to the circuit acting on all qubits\n",
        "    qc.append(qc_inst, range(num_qubits))\n",
        "\n",
        "    # Return the complete pooling layer circuit\n",
        "    return qc\n",
        "\n",
        "\n",
        "# Define source qubits indices (e.g., [0, 1])\n",
        "sources = [0, 1]\n",
        "\n",
        "# Define sink qubits indices (e.g., [2, 3])\n",
        "sinks = [2, 3]\n",
        "\n",
        "# Instantiate the pooling layer circuit for the given qubits and parameters prefix \"θ\"\n",
        "circuit = pool_layer(sources, sinks, \"θ\")\n",
        "\n",
        "# Decompose the circuit to show individual gates and draw it with matplotlib using clifford style\n",
        "circuit.decompose().draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafef540"
      },
      "source": [
        "In this particular example, we reduce the dimensionality of our four qubit circuit to the last two qubits, i.e. the last two qubits in this particular example. These qubits are then used in the next layer, while the first two are neglected for the remainder of the QCNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03d0497d"
      },
      "source": [
        "### 1.2.3. Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c3fbdc"
      },
      "source": [
        "One common use of a CCNN is an image classifier, where a CCNN detects particular features and patterns (such as straight lines or curves) of the pixelated images through the use of the feature maps in the convolutional layer. By learning the relationship between these features, it can then classify and label handwritten digits with ease.\n",
        "\n",
        "Because of a classical CNN's ability to recognize features and patterns easily, we will train our QCNN to also determine patterns and features of a given set of pixelated images, and classify between two different patterns.\n",
        "\n",
        "To simplify the dataset, we only consider 2 x 4 pixelated images. The patterns we will train the QCNN to distinguish will be a horizontal or vertical line, which can be placed anywhere in the image, alongside a noisy background.\n",
        "\n",
        "We first begin by generating this dataset. To create a 'horizontal' or 'vertical' line, we assign pixels value to be $\\frac{\\pi}{2}$ which will represent the line in our pixelated image. We create a noisy background by assigning every other pixel a random value between $0$ and $\\frac{\\pi}{4}$ which will create a noisy background.\n",
        "\n",
        "Note that when we create our dataset, we need to split it into the training set and testing set of images, the datasets we train and test our neural network respectively.\n",
        "\n",
        "We also need to label our datasets such that the QCNN can learn to differentiate between the two patterns. In this example we label images with a horizontal line with -1 and images with a vertical line +1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5S3fXdKKDh1J"
      },
      "outputs": [],
      "source": [
        "# Define a function to generate a dataset of quantum feature vectors and corresponding labels\n",
        "def generate_dataset(num_images):\n",
        "    images = []  # List to store generated image feature vectors\n",
        "    labels = []  # List to store class labels for each image\n",
        "\n",
        "    # Initialize two arrays representing horizontal and vertical \"images\"\n",
        "    hor_array = np.zeros((6, 8))  # 6 horizontal patterns, each of length 8\n",
        "    ver_array = np.zeros((4, 8))  # 4 vertical patterns, each of length 8\n",
        "\n",
        "    # Fill hor_array with π/2 values in adjacent pairs for horizontal line patterns,\n",
        "    # skipping the middle index 3 to avoid overlap\n",
        "    j = 0\n",
        "    for i in range(0, 7):\n",
        "        if i != 3:\n",
        "            hor_array[j][i] = np.pi / 2       # Set rotation angle π/2 at position i\n",
        "            hor_array[j][i + 1] = np.pi / 2   # Also set π/2 at adjacent position i+1\n",
        "            j += 1\n",
        "\n",
        "    # Fill ver_array with π/2 values split into two groups of 4 positions,\n",
        "    # representing vertical line patterns\n",
        "    j = 0\n",
        "    for i in range(0, 4):\n",
        "        ver_array[j][i] = np.pi / 2          # Set π/2 at position i in the first half\n",
        "        ver_array[j][i + 4] = np.pi / 2      # Set π/2 at corresponding position in second half\n",
        "        j += 1\n",
        "\n",
        "    # Generate num_images random images with labels\n",
        "    for n in range(num_images):\n",
        "        # Randomly choose class: 0 for horizontal (-1 label), 1 for vertical (+1 label)\n",
        "        rng = algorithm_globals.random.integers(0, 2)\n",
        "        if rng == 0:\n",
        "            labels.append(-1)  # Label for horizontal pattern\n",
        "            random_image = algorithm_globals.random.integers(0, 6)  # Pick random horizontal pattern\n",
        "            images.append(np.array(hor_array[random_image]))        # Append the pattern to images list\n",
        "        elif rng == 1:\n",
        "            labels.append(1)   # Label for vertical pattern\n",
        "            random_image = algorithm_globals.random.integers(0, 4)  # Pick random vertical pattern\n",
        "            images.append(np.array(ver_array[random_image]))        # Append the pattern to images list\n",
        "\n",
        "        # Add noise to zero-valued entries in the last appended image\n",
        "        for i in range(8):\n",
        "            if images[-1][i] == 0:\n",
        "                # Replace zeros with a random value uniformly sampled between 0 and π/4\n",
        "                images[-1][i] = algorithm_globals.random.uniform(0, np.pi / 4)\n",
        "\n",
        "    # Return the list of generated images (feature vectors) and their corresponding labels\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5b3ca82"
      },
      "source": [
        "Let's now create our dataset below and split it into our test and training datasets. We pass a `random_state` so the split will be the same each time this notebook is run so the final results do not vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i-sYu3XVDoJ9"
      },
      "outputs": [],
      "source": [
        "# Generate a dataset of 50 quantum feature vectors (images) and their labels using the custom generator function\n",
        "images, labels = generate_dataset(50)\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "# - 70% of the data will be used for training, 30% for testing\n",
        "# - random_state=246 ensures reproducibility of the split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.3, random_state=246\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f6952d"
      },
      "source": [
        "Let's see some examples in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCPyYlE6D61p"
      },
      "outputs": [],
      "source": [
        "# Create a 2x2 grid of subplots with specified figure size\n",
        "# Disable x and y ticks on all subplots for a cleaner image display\n",
        "fig, ax = plt.subplots(2, 2, figsize=(10, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
        "\n",
        "# Loop through the first 4 training images\n",
        "for i in range(4):\n",
        "    # Reshape each 1D image vector back into a 2x4 matrix for visualization\n",
        "    # Display the reshaped image in the corresponding subplot\n",
        "    ax[i // 2, i % 2].imshow(\n",
        "        train_images[i].reshape(2, 4),\n",
        "        aspect=\"equal\",  # Keep pixels square\n",
        "    )\n",
        "\n",
        "# Adjust spacing between subplots for better layout\n",
        "plt.subplots_adjust(wspace=0.1, hspace=0.025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a67058"
      },
      "source": [
        "As we can see each image contains either a vertical or horizontal line, that the QCNN will learn how to differentiate. Now that we have built our dataset, it is time to discuss the components of the QCNN and build our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eed5d2d6"
      },
      "source": [
        "### 1.2.4. Modeling our QCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64efb8d8"
      },
      "source": [
        "Now that we have defined both the convolutional layers it is now time to build our QCNN, which will consist of alternating pooling and convolutional layers.\n",
        "\n",
        "As the images in our dataset contains 8 pixels, we will use 8 qubits in our QCNN.\n",
        "\n",
        "We encode our dataset into our QCNN by applying a feature map. One can create a feature map using one of Qiskit's built in feature maps, such as ZFeatureMap or ZZFeatureMap.\n",
        "\n",
        "After analyzing several different Feature maps for this dataset, it was found that QCNN obtains the greatest accuracy when the Z feature map is used. Therefore, throughout the remainder of the tutorial we will use the Z feature Map, of which can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7f8Ev9FECpI"
      },
      "outputs": [],
      "source": [
        "# Create a quantum feature map circuit using Z rotations on 8 qubits\n",
        "feature_map = ZFeatureMap(8)\n",
        "\n",
        "# Decompose the feature map circuit into its elementary gates\n",
        "# and draw it using matplotlib with the 'clifford' style for clear visualization\n",
        "feature_map.decompose().draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cee9549c"
      },
      "source": [
        "We create a function for our QCNN, which will contain three sets of alternating convolutional and pooling layers, which can be seen in the schematic below. Through the use of the pooling layers, we thus reduce the dimensionality of our QCNN from eight qubits to one.\n",
        "\n",
        "<a href=\"https://ibb.co/xtCtmH5d\"><img src=\"https://i.ibb.co/ycfchSs7/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "To classify our image dataset of horizontal and vertical lines, we measure the expectation value of the Pauli Z operator of the final qubit. Based on the obtained value being +1 or -1, we can conclude that the input image contained either a horizontal or vertical line.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e930ed08"
      },
      "source": [
        "### 1.2.5. Training our QCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e04e424"
      },
      "source": [
        "The next step is to build our model using our training data.\n",
        "\n",
        "To classify our system, we perform a measurement from the output circuit. The value we obtain will thus classify whether our input data contains either a vertical line or horizontal line.\n",
        "\n",
        "The measurement we have chosen in this tutorial is $<Z>$, i.e. the expectation value of the Pauli Z qubit for the final qubit. Measuring this expectation value, we obtain +1 or -1, which correspond to a vertical or horizontal line respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM09iYujE-aK"
      },
      "outputs": [],
      "source": [
        "# Create an 8-qubit ZFeatureMap to encode classical data via Z rotations on each qubit\n",
        "feature_map = ZFeatureMap(8)\n",
        "\n",
        "# Initialize an empty 8-qubit quantum circuit named \"Ansatz\" for the trainable layers\n",
        "ansatz = QuantumCircuit(8, name=\"Ansatz\")\n",
        "\n",
        "# Compose the first convolutional layer on all 8 qubits using parameters prefixed \"c1\"\n",
        "ansatz.compose(conv_layer(8, \"c1\"), list(range(8)), inplace=True)\n",
        "\n",
        "# Compose the first pooling layer acting on qubit pairs ([0,1,2,3] as sources and [4,5,6,7] as sinks)\n",
        "ansatz.compose(pool_layer([0, 1, 2, 3], [4, 5, 6, 7], \"p1\"), list(range(8)), inplace=True)\n",
        "\n",
        "# Compose the second convolutional layer on the upper 4 qubits (4 through 7) with parameters \"c2\"\n",
        "ansatz.compose(conv_layer(4, \"c2\"), list(range(4, 8)), inplace=True)\n",
        "\n",
        "# Compose the second pooling layer on qubits 4 to 7 with sources [0,1] and sinks [2,3] relative to that subset\n",
        "ansatz.compose(pool_layer([0, 1], [2, 3], \"p2\"), list(range(4, 8)), inplace=True)\n",
        "\n",
        "# Compose the third convolutional layer on last two qubits (6 and 7) with parameters \"c3\"\n",
        "ansatz.compose(conv_layer(2, \"c3\"), list(range(6, 8)), inplace=True)\n",
        "\n",
        "# Compose the third pooling layer on qubits 6 and 7 with source 0 and sink 1 relative to that subset\n",
        "ansatz.compose(pool_layer([0], [1], \"p3\"), list(range(6, 8)), inplace=True)\n",
        "\n",
        "# Create a new 8-qubit circuit to combine feature map and ansatz\n",
        "circuit = QuantumCircuit(8)\n",
        "# Append the feature map to the circuit\n",
        "circuit.compose(feature_map, range(8), inplace=True)\n",
        "# Append the ansatz after the feature map\n",
        "circuit.compose(ansatz, range(8), inplace=True)\n",
        "\n",
        "# Define the observable as a Pauli Z operator acting on the first qubit (others identity)\n",
        "observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * 7, 1)])\n",
        "\n",
        "# Decompose the circuit to simplify it and avoid data copying overhead in QNN\n",
        "# Create an EstimatorQNN object specifying the circuit, observable, and parameters\n",
        "qnn = EstimatorQNN(\n",
        "    circuit=circuit.decompose(),\n",
        "    observables=observable,\n",
        "    input_params=feature_map.parameters,\n",
        "    weight_params=ansatz.parameters,\n",
        "    estimator=estimator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipkgwbq-FJjv"
      },
      "outputs": [],
      "source": [
        "# Draw the full quantum circuit combining feature map and ansatz\n",
        "# Use matplotlib rendering with 'clifford' style for clear, standardized gate visuals\n",
        "circuit.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7ec49c"
      },
      "source": [
        "We will also define a callback function to use when training our model. This allows us to view and plot the loss function per each iteration in our training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ifwI1DwkFPi2"
      },
      "outputs": [],
      "source": [
        "# Define a callback function to track and visualize training progress during optimization\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    # Clear previous output to update plot dynamically without flooding output cells\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Append the current objective function evaluation (loss) to the list for plotting\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "\n",
        "    # Set plot title and axis labels\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "\n",
        "    # Plot the objective function values over all completed iterations\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "\n",
        "    # Display the updated plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4371b5a7"
      },
      "source": [
        "In this example, we will use the COBYLA optimizer to train our classifier, which is a numerical optimization method commonly used for classification machine learning algorithms.\n",
        "\n",
        "We then place the the callback function, optimizer and operator of our QCNN created above into Qiskit Machine Learning's built in Neural Network Classifier, which we can then use to train our model.\n",
        "\n",
        "Since model training may take a long time we have already pre-trained the model for some iterations and saved the pre-trained weights. We'll continue training from that point by setting `initial_point` to a vector of pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z2xXrzq9Frih"
      },
      "outputs": [],
      "source": [
        "# Open a JSON file containing precomputed initial parameter values for the classifier\n",
        "with open(\"11_qcnn_initial_point.json\", \"r\") as f:\n",
        "    initial_point = json.load(f)\n",
        "\n",
        "# Create a NeuralNetworkClassifier using the quantum neural network 'qnn'\n",
        "# - Use COBYLA optimizer with a maximum of 200 iterations\n",
        "# - Use 'callback_graph' to visualize training progress dynamically\n",
        "# - Initialize the optimizer parameters with values from 'initial_point' to potentially improve convergence\n",
        "classifier = NeuralNetworkClassifier(\n",
        "    qnn,\n",
        "    optimizer=COBYLA(maxiter=200),\n",
        "    callback=callback_graph,\n",
        "    initial_point=initial_point,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9061806"
      },
      "source": [
        "After creating this classifier, we can train our QCNN using our training dataset and each image's corresponding label. Because we previously defined the callback function, we plot the overall loss of our system per iteration.\n",
        "\n",
        "It may take some time to train the QCNN so be patient!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBwgh8RjGCXU"
      },
      "outputs": [],
      "source": [
        "# Convert training images and labels to NumPy arrays for compatibility with scikit-learn interface\n",
        "x = np.asarray(train_images)\n",
        "y = np.asarray(train_labels)\n",
        "\n",
        "# Initialize a list to store objective function values during training for visualization\n",
        "objective_func_vals = []\n",
        "\n",
        "# Set default figure size for training loss plots\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train the quantum neural network classifier using the training data\n",
        "classifier.fit(x, y)\n",
        "\n",
        "# Evaluate and print the classification accuracy on the training dataset, rounded to two decimals\n",
        "print(f\"Accuracy from the train data : {np.round(100 * classifier.score(x, y), 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95d1100"
      },
      "source": [
        "As we can see from above, the QCNN converges slowly, hence our `initial_point` was already close to an optimal solution. The next step is to determine whether our QCNN can classify data seen in our test image data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72b3cf40"
      },
      "source": [
        "### 1.2.6. Testing our QCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571b1b32"
      },
      "source": [
        "After building and training our dataset we now test whether our QCNN can predict images that are not from our test data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVOKJzCrGQn3"
      },
      "outputs": [],
      "source": [
        "# Use the trained classifier to predict labels for the test images\n",
        "y_predict = classifier.predict(test_images)\n",
        "\n",
        "# Convert test images and labels to NumPy arrays for scoring\n",
        "x = np.asarray(test_images)\n",
        "y = np.asarray(test_labels)\n",
        "\n",
        "# Print the classification accuracy on the test dataset, rounded to two decimals\n",
        "print(f\"Accuracy from the test data : {np.round(100 * classifier.score(x, y), 2)}%\")\n",
        "\n",
        "# Visualize 4 test images along with the QCNN's predictions\n",
        "fig, ax = plt.subplots(2, 2, figsize=(10, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
        "\n",
        "# Loop over the first 4 test images\n",
        "for i in range(0, 4):\n",
        "    # Reshape each image back into 2x4 format for visualization\n",
        "    ax[i // 2, i % 2].imshow(test_images[i].reshape(2, 4), aspect=\"equal\")\n",
        "\n",
        "    # Set subplot title based on the predicted label: -1 means Horizontal Line, +1 means Vertical Line\n",
        "    if y_predict[i] == -1:\n",
        "        ax[i // 2, i % 2].set_title(\"The QCNN predicts this is a Horizontal Line\")\n",
        "    if y_predict[i] == +1:\n",
        "        ax[i // 2, i % 2].set_title(\"The QCNN predicts this is a Vertical Line\")\n",
        "\n",
        "# Adjust spacing between subplots for clarity\n",
        "plt.subplots_adjust(wspace=0.1, hspace=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f296307"
      },
      "source": [
        "From above, we can indeed see that our QCNN can classify horizontal and vertical lines! Congratulations! Through the use of quantum circuits and quantum convolutional and pooling layers, you have built a Quantum Convolutional Neural Network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrk9b-HEalCu"
      },
      "source": [
        "## 2. Quantum Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1764d89"
      },
      "source": [
        "The goal of this tutorial is to build an Quantum Autoencoder, a circuit which can compress a quantum state onto a smaller amount of qubits, while retaining the information from the initial state.\n",
        "\n",
        "Throughout this tutorial, we explain the architecture of a Quantum Autoencoder and how one can design and train such a system to compress and encode information. Following this discussion, we give two examples to demonstrate the capabilities of such a system to compress different quantum states, as well as the ability to compress images of zeros and ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2af97494"
      },
      "source": [
        "### 2.1. What is an Autoencoder?\n",
        "\n",
        "A classical autoencoder (CAE) is a type of neural network architecture that is commonly used to efficiently compress and encode information from the input using of representation learning. Following compression, one can then uncompress the data through the use of a decoder.\n",
        "\n",
        "Typical autoencoders are commonly divided into three layers, as seen in Figure 1.\n",
        "\n",
        "\n",
        "The first layer is called the Input Layer (1) and is the layer of which we input our data of length $n$.\n",
        "\n",
        "The input data then passes through an encoder and travels to the next layer, which has less nodes or is reduced in dimensions and is known as the Bottleneck Layer (2). The input layer is compressed through this process. Common CAEs may have several layers.\n",
        "\n",
        "The final layer is called the Output Layer (3). Here the compressed data is reconstructed to its original size, $n$, from the compressed data through the process of a decoder.\n",
        "\n",
        "By passing our input data through a CAE, we are therefore able to reduce the dimensionality of our input data, as seen in the bottleneck layer, while retaining as much information as possible from the input data. Because of this feature, common uses of CAE are Image Denoising, Anomaly Detection and Facial Recognition devices. For more information on classical autoencoders.\n",
        "\n",
        "<a href=\"https://ibb.co/cc13r0q9\"><img src=\"https://i.ibb.co/xSDL7WZb/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66031d83"
      },
      "source": [
        "### 2.2. The Quantum Autoencoder\n",
        "\n",
        "We can also define a quantum counterpart to the CAE, the Quantum Autoencoder. Much like the CAE, the Quantum Autoencoder aims to reduce the dimensionality of the input of the neural network, in this case a quantum state. A pictorial representation of this can be seen in Figure 2.\n",
        "\n",
        "<a href=\"https://ibb.co/kgY5G9d5\"><img src=\"https://i.ibb.co/hRpLgZqL/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 2: Pictorial Representation of a Quantum Autoencoder. Here one can see the similarities with the CAE, with the circuit having an input state, bottleneck state and an output state.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Much like its classical counterpart, our circuit contains three layers. We first input our state $|\\psi>$ (which contains $n$ qubits), of which we wish to compress. This is our input layer (1).\n",
        "\n",
        "We then apply our parametrized circuit on our input state, which will act as our encoder and 'compresses' our quantum state, reducing the dimensionality of our state to $n-k$ qubits. Our new compressed state is of the form $|\\psi_{comp}> \\otimes |0>^{\\otimes k}$, where $|\\psi_{comp}>$ contains $n-k$ qubits.\n",
        "\n",
        "This parametrized circuit will depend on a set of parameters, which will be the nodes of our Quantum Autoencoder. Throughout the training process, these parameters will be updated to optimize the loss function.\n",
        "\n",
        "We disregard the remaining $k$ qubits for the remainder of the circuit. This is our bottleneck layer (2) and our input state is now compressed.\n",
        "\n",
        "The final layer consists of the addition of $k$ qubits (all in the state $|0\\rangle$) and applying another parametrized circuit between the compressed state and the new qubits. This parametrized circuit acts as our decoder and reconstructs the input state from the compressed state using the new qubits. After the decoder, we retain the original state as the state travels to the output layer (3).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f4aae55"
      },
      "source": [
        "### 2.3. Components of a Quantum Autoencoder\n",
        "\n",
        "Before building our Quantum Autoencoder, we must note a few subtleties.\n",
        "\n",
        "We first note that we cannot introduce or disregard qubits in the middle of a Quantum Circuit when implementing an autoencoder using Qiskit.\n",
        "\n",
        "Because of this we must include our reference state as well as our auxiliary qubits (whose role will be described in later sections) at the beginning of the circuit.\n",
        "\n",
        "Therefore our input state will consist of our input state, reference state and one auxiliary qubit, as well as a classical register to perform measurements (which will be described in the next section). A pictorial representation of this can be seen in Figure 3.\n",
        "\n",
        "<a href=\"https://ibb.co/rKyQscGx\"><img src=\"https://i.ibb.co/GfPdTtvW/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 3: Pictorial Representation of input state of Quantum Autoencoder. Note that we must also include an auxiliary qubit, the reference state and classical register at the beginning of the circuit, even though they are not used until later in the circuit.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6186d9a"
      },
      "source": [
        "### 2.4. Choosing a Loss Function\n",
        "\n",
        "We now define our cost function, which we will use to train our Quantum Autoencoder, to return the input state. There's a bit of math involved here, so skip this section if you're not interested!\n",
        "\n",
        "We take the cost function as defined in [2], which tries to maximize the fidelity between the input and output state of our Quantum Autoencoder.\n",
        "\n",
        "We first define subsystems $A$ and $B$ to contain $n$ and $k$ qubits respectively, while $B'$ is the space which will contain our reference space. We call the subsystem $A$ our latent space, which will contain the compressed qubit state, and $B$ our trash space, which contain the qubits of which we disregard throughout compression.\n",
        "\n",
        "Our input state therefore $|\\psi_{AB}>$ contains $n + k$ qubits. We define the reference space $B'$ which contains the reference state $|a>_{B'}$. This space will contain the additional $k$ qubits we use in the decoder. All of these subsystems can be seen in Figure 3.\n",
        "\n",
        "We define the parameterized circuit as $U(\\theta)$ which we will use as our encoder. However the structure and parameters of our parametrized circuit is currently unknown to us and may vary for different input states. To determine the parameters to compress our input state, we must train our device to maximally compress the state by adjusting the values of the parameters $\\theta$. For the decoder we will use $U^{\\dagger}(\\theta)$.\n",
        "\n",
        "Our goal therefore is to maximize the fidelity between the input and output states, i.e.\n",
        "\n",
        "$$\\text{max }F(\\psi_{AB}, \\rho_{out})$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\rho_{out} = U^{\\dagger}(\\theta)_{AB'} \\text{Tr}_{B} [U(\\theta)_{AB}[\\psi_{AB} \\otimes a_{B'}]U^{\\dagger}(\\theta)_{AB}]U(\\theta)_{AB'}$$\n",
        "\n",
        "We can maximize this fidelity by tuning the parameters $\\theta$ in our parametrized circuit. However, this fidelity can at times be complicated to determine and may require a large amount of gates needed to calculate the fidelity between two states, i.e. the larger the number of qubits, the more gates required which results to deeper circuits.  Therefore we look for alternative means of comparing the input and output states.\n",
        "\n",
        "As shown in [2] a simpler way of determining an optimally compressed state is to perform a swap gate between the trash state and reference state. These states usually have a smaller number of qubits and are therefore easier to compare, due to the smaller amount of gates required. As shown in [2] maximizing the fidelity of such these two states is equivalent to maximizing the fidelity of the input and output state and thus determining an optimal compression of our input circuit.\n",
        "\n",
        "Keeping our reference state fixed, our cost function will now be a function of the trash state and is denoted as;\n",
        "\n",
        "$$\\text{max }F(\\text{Tr}_{A} [ U(\\theta)_{AB}\\psi_{AB} U^{\\dagger}(\\theta)_{AB}], a_{B'})$$\n",
        "\n",
        "Throughout the training process, we adjust the parameters $\\theta$ in our encoder and perform a swap test (as described below) to determine the fidelity between these trash and reference states. In doing so, we must include an additional qubit, our auxiliary qubit, which will be used throughout the swap test and measured to determine the overall fidelity of the trash and reference states. This is the reason why we included both an auxiliary qubit and classical register in the previous section when initializing our circuit.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721636a1"
      },
      "source": [
        "### 2.5. The SWAP Test\n",
        "\n",
        "The SWAP Test is a procedure commonly used to compare two states by applying CNOT gates to each qubit (for further information see [3]). By running the circuit $M$ times, and applying the SWAP test, we then measure the auxiliary qubit. We use the number of states in the state $|1\\rangle$ to compute:\n",
        "\n",
        "$$S = 1 - \\frac{2}{M}L$$\n",
        "\n",
        "where $L$ is the count for the states in the $|1\\rangle$ state. As shown in [3], maximizing this function corresponds to the two states of which we are comparing being identical. We therefore aim to maximize this function, i.e. minimize  $\\frac{2}{M}L$. This value will be therefore be our cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa17e37a"
      },
      "source": [
        "### 2.6. Building the Quantum Autoencoder Ansatz\n",
        "\n",
        "First, we implement IBM's Qiskit to build our Quantum Autoencoder. We first begin by importing in the necessary libraries and fixing the seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "j7gb3vQirAnV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from qiskit import ClassicalRegister, QuantumRegister\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import RealAmplitudes\n",
        "from qiskit.primitives import StatevectorSampler as Sampler\n",
        "from qiskit.quantum_info import Statevector\n",
        "from qiskit_machine_learning.optimizers import COBYLA\n",
        "from qiskit_machine_learning.utils import algorithm_globals\n",
        "\n",
        "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "\n",
        "algorithm_globals.random_seed = 42\n",
        "sampler = Sampler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytGC4iHIsJhl"
      },
      "source": [
        "We begin by defining our parametrized ansatz for the Quantum Autoencoder. This will be our parametrized circuit where we can tune the parameters to maximize the fidelity between the trash and reference states.\n",
        "\n",
        "### 2.7. The Parametrized Circuit\n",
        "\n",
        "The parametrized circuit we will use below for our encoder is the RealAmplitude Ansatz available in Qiskit. One of the reasons why we have chosen this ansatz is because it is a 2-local circuit, the prepared quantum states will only have real amplitudes, and does not rely on full connectivity between each qubits, which is hard to implement or can lead to deep circuits.\n",
        "\n",
        "We define our parametrized circuit for our Encoder below, where we set the repetition parameter to `reps=5`, to increase the number of parameters in our circuit allowing greater flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NpiRzpjRsLX0"
      },
      "outputs": [],
      "source": [
        "# Define a function named 'ansatz' that creates a parameterized quantum circuit\n",
        "# The circuit uses the RealAmplitudes ansatz with the specified number of qubits\n",
        "# 'reps=5' means the ansatz layers (rotation + entanglement) are repeated 5 times\n",
        "def ansatz(num_qubits):\n",
        "    return RealAmplitudes(num_qubits, reps=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrDAitysL-E"
      },
      "source": [
        "Let's draw this ansatz with $5$ qubits and see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNqRQAdosNwp"
      },
      "outputs": [],
      "source": [
        "# Set the number of qubits for the ansatz circuit to 5\n",
        "num_qubits = 5\n",
        "\n",
        "# Generate the ansatz circuit using the previously defined function\n",
        "circ = ansatz(num_qubits)\n",
        "\n",
        "# Decompose the ansatz circuit into elementary gates for detailed visualization\n",
        "# Draw the decomposed circuit using matplotlib with the 'clifford' style for clarity\n",
        "circ.decompose().draw(output=\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8925b02"
      },
      "source": [
        "We now apply this Encoder to the state we wish to compress. In this example, we divide our initial $5$ qubit state into a $3$ qubit latent state ($n = 3$) and $2$ qubit trash space ($k = 2$).\n",
        "\n",
        "As explained in the previous section, we must also include a $2$ qubit reference space in our circuit, as well as an auxiliary qubit to perform the swap test between the reference and trash states. We will therefore have a total of $2 + 3 + 2 + 1 = 8$ qubits and $1$ classical register in our circuit.\n",
        "\n",
        "After initializing our state, we apply our parametrized circuit.\n",
        "\n",
        "Following this, we then split our initial state into the latent space (the compressed state) and trash space (the part of the state we will disregard) and perform the swap test between the reference state and the trash space. The last qubit is then measured to determine the fidelity between the reference and trash states.  A pictorial representation of this is given below in Figure 4.\n",
        "\n",
        "<a href=\"https://ibb.co/DHmh1Gmp\"><img src=\"https://i.ibb.co/Q3hLJ8hd/image.png\" alt=\"image\" border=\"0\"></a><br /><a target='_blank' href='https://nl.imgbb.com/'></a><br />\n",
        "\n",
        "*Figure 4: Example of a Quantum Autoencoder in the training process. We use the swap test to determine the fidelity between the trash and reference space.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24d20fb"
      },
      "source": [
        "We define a function below to implement the above circuit configuration to the $5$ qubit domain wall state $|00111\\rangle$ and plot an example below. Here qubits $5$ and $6$ are the reference state, $0, 1, 2, 3, 4$ are the initial state we wish to compress and qubit $7$ is our auxiliary qubit which is used in the swap test. We also include a classical register to measure the results of qubit $7$ in the swap test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDNMkMopsUw8"
      },
      "outputs": [],
      "source": [
        "# Define a function to create an autoencoder quantum circuit with latent and trash qubits\n",
        "def auto_encoder_circuit(num_latent, num_trash):\n",
        "    # Create a quantum register with total qubits: latent + 2 * trash + 1 auxiliary qubit\n",
        "    qr = QuantumRegister(num_latent + 2 * num_trash + 1, \"q\")\n",
        "\n",
        "    # Create a classical register with 1 bit for measurement\n",
        "    cr = ClassicalRegister(1, \"c\")\n",
        "\n",
        "    # Initialize the quantum circuit with these quantum and classical registers\n",
        "    circuit = QuantumCircuit(qr, cr)\n",
        "\n",
        "    # Compose the ansatz circuit on the first (num_latent + num_trash) qubits in-place\n",
        "    circuit.compose(ansatz(num_latent + num_trash), range(0, num_latent + num_trash), inplace=True)\n",
        "\n",
        "    # Insert a barrier to visually separate circuit sections\n",
        "    circuit.barrier()\n",
        "\n",
        "    # Define the index of the auxiliary qubit used for the swap test\n",
        "    auxiliary_qubit = num_latent + 2 * num_trash\n",
        "\n",
        "    # Begin the swap test by applying a Hadamard gate to the auxiliary qubit\n",
        "    circuit.h(auxiliary_qubit)\n",
        "\n",
        "    # Perform controlled swap operations between pairs of trash qubits controlled by the auxiliary qubit\n",
        "    for i in range(num_trash):\n",
        "        # Controlled SWAP between qubits: auxiliary controls swapping num_latent + i and num_latent + num_trash + i\n",
        "        circuit.cswap(auxiliary_qubit, num_latent + i, num_latent + num_trash + i)\n",
        "\n",
        "    # Apply another Hadamard gate to the auxiliary qubit to complete the swap test\n",
        "    circuit.h(auxiliary_qubit)\n",
        "\n",
        "    # Measure the auxiliary qubit and store the result in the classical register\n",
        "    circuit.measure(auxiliary_qubit, cr[0])\n",
        "\n",
        "    # Return the complete autoencoder circuit\n",
        "    return circuit\n",
        "\n",
        "# Define the number of latent qubits and trash qubits\n",
        "num_latent = 3\n",
        "num_trash = 2\n",
        "\n",
        "# Create the autoencoder circuit with specified latent and trash qubits\n",
        "circuit = auto_encoder_circuit(num_latent, num_trash)\n",
        "\n",
        "# Draw the circuit using matplotlib with 'clifford' style for clear visualization\n",
        "circuit.draw(output=\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c0bc911"
      },
      "source": [
        "In order to reconstruct the original input state, we must apply the adjoint of our parametrized circuit after the swap test. However, during training, we are only interested in the trash state and the reference state. We can therefore exclude the gates following compression until we wish to reconstruct our initial input.\n",
        "\n",
        "After building our Quantum Autoencoder, the next step is to train our Quantum Autoencoder to compress the state and maximize the cost function and determine the parameters $\\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc886404"
      },
      "source": [
        "### 2.8. A Simple Example: The Domain Wall Autoencoder\n",
        "\n",
        "Let's first begin with a simple example, a state known as the Domain Wall, which for $5$ qubits is given by $|00111\\rangle$. Here we will try and compress this state from $5$ qubits to $3$ qubits, with the remaining qubits in the trash space, in the state $|00\\rangle$. We can create a function to build the domain wall state below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fjH9uWVsY1i"
      },
      "outputs": [],
      "source": [
        "# Define a function to apply a Domain Wall pattern on qubits from a to b in the given circuit\n",
        "def domain_wall(circuit, a, b):\n",
        "    # For qubits in the second half of the range [a, b), apply an X (NOT) gate\n",
        "    for i in np.arange(int(b / 2), int(b)):\n",
        "        circuit.x(i)\n",
        "    # Return the modified circuit with Domain Wall applied\n",
        "    return circuit\n",
        "\n",
        "# Create a 5-qubit quantum circuit and apply the Domain Wall pattern from qubit 0 to 5\n",
        "domain_wall_circuit = domain_wall(QuantumCircuit(5), 0, 5)\n",
        "\n",
        "# Draw the resulting circuit using matplotlib with the 'clifford' style for clarity\n",
        "domain_wall_circuit.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcc66776"
      },
      "source": [
        "Now let's train our Autoencoder to compress this state from 5 qubits to 3 qubits (qubits 0,1 and 2), with the remaining qubits in the trash space (qubits 3 and 4) being in the |00> state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8442b1"
      },
      "source": [
        "We create a circuit to be used in the loss function, as described in Section 4, which determines the fidelity between the two states below using the swap test for our particular AutoEncoder function. For further information on the swap test, see [1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KSNUi34sb2k"
      },
      "outputs": [],
      "source": [
        "# Create the autoencoder circuit with specified latent and trash qubits\n",
        "ae = auto_encoder_circuit(num_latent, num_trash)\n",
        "\n",
        "# Initialize a new quantum circuit with the total number of qubits and one classical bit for measurement\n",
        "qc = QuantumCircuit(num_latent + 2 * num_trash + 1, 1)\n",
        "\n",
        "# Compose the domain wall circuit on the first (num_latent + num_trash) qubits of qc\n",
        "qc = qc.compose(domain_wall_circuit, range(num_latent + num_trash))\n",
        "\n",
        "# Compose the autoencoder circuit 'ae' onto 'qc'\n",
        "qc = qc.compose(ae)\n",
        "\n",
        "# Draw the combined quantum circuit using matplotlib with the 'clifford' style for visualization\n",
        "qc.draw(output=\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reasonable-distributor"
      },
      "source": [
        "Then, we create a quantum neural network and pass the circuit as a parameter. We note that this network must take an interpret function, which determines how we map the output of the network to the output shape. Since we measure only one qubit, the output of the network is a bit string either $0$ or $1$, so the output shape is $2$, the number of possible outcomes. Then, we introduce an identity mapping. The output of the network is a vector of probabilities of getting interpret-mapped bit strings. Thus, we get probabilities of getting $0$ or $1$ and this is exactly what we are looking for. In the cost function we make use of the probability of getting $1$ and penalize the outcomes that lead to $1$, therefore maximizing the fidelity between the trash space and the reference space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK8jEWoOsd0F"
      },
      "outputs": [],
      "source": [
        "# Define an identity interpretation function that returns measurement outcomes unchanged\n",
        "def identity_interpret(x):\n",
        "    return x\n",
        "\n",
        "# Create a SamplerQNN instance with the following settings:\n",
        "qnn = SamplerQNN(\n",
        "    circuit=qc,               # Use the composed quantum circuit 'qc'\n",
        "    input_params=[],          # No input parameters (empty list) as data is embedded within the circuit\n",
        "    weight_params=ae.parameters,  # Trainable weight parameters from the autoencoder circuit\n",
        "    interpret=identity_interpret, # Use the identity interpret function to keep raw sampler outputs\n",
        "    output_shape=2,           # Define the shape of the output space (e.g., binary outputs)\n",
        "    sampler=sampler,          # Use the quantum sampler primitive to execute circuit and obtain samples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0fea32"
      },
      "source": [
        "Next we create our cost function. As described in the previous section, our aim is to minimize $\\frac{2}{M}L$, which is the twice the probability of getting the final qubit in the $|1\\rangle$ state. We therefore wish to minimize the of getting a $|1\\rangle$ on qubit 7.\n",
        "\n",
        "The cost function will also plot out the objective value at each cost function evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Hhkpji3CsgTD"
      },
      "outputs": [],
      "source": [
        "# Define a cost function to be minimized during training using parameter values\n",
        "def cost_func_domain(params_values):\n",
        "    # Perform a forward pass on the quantum neural network 'qnn' with the given weights (params_values)\n",
        "    # No input parameters are passed (empty list) because circuit inputs are fixed\n",
        "    probabilities = qnn.forward([], params_values)\n",
        "\n",
        "    # Calculate the cost as the sum of probabilities of measuring '1' in the output bitstring\n",
        "    cost = np.sum(probabilities[:, 1])\n",
        "\n",
        "    # Clear previous output to update the plot dynamically during optimization\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Append the current cost value to the list for tracking\n",
        "    objective_func_vals.append(cost)\n",
        "\n",
        "    # Plot the objective function value progression against iteration count\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()\n",
        "\n",
        "    # Return the current cost value for the optimizer\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c97545c2"
      },
      "source": [
        "Now we will train our Autoencoder to reduce the dimension of the Hilbert space from $5$ qubits to $3$, while leaving the trash space in the state $|00\\rangle$.  We initially set the parameters $\\theta$ to random values and tune these parameters to minimize our cost function through the use of the COBYLA optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMNritErsiGk"
      },
      "outputs": [],
      "source": [
        "# Initialize the COBYLA classical optimizer with a maximum of 150 iterations\n",
        "opt = COBYLA(maxiter=150)\n",
        "\n",
        "# Generate a random initial point for the parameters of the autoencoder circuit\n",
        "initial_point = algorithm_globals.random.random(ae.num_parameters)\n",
        "\n",
        "# Initialize an empty list to store objective function values during optimization for plotting\n",
        "objective_func_vals = []\n",
        "\n",
        "# Set figure size for plots to improve readability\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Record the start time for optimization\n",
        "start = time.time()\n",
        "\n",
        "# Run the optimization to minimize the cost function starting from the initial random parameters\n",
        "opt_result = opt.minimize(cost_func_domain, initial_point)\n",
        "\n",
        "# Calculate the elapsed time taken for optimization\n",
        "elapsed = time.time() - start\n",
        "\n",
        "# Print the total time taken for the fitting/training process\n",
        "print(f\"Fit in {elapsed:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0242ad6d"
      },
      "source": [
        "Looks like it has converged! After training our Quantum Autoencoder, let's build it and see how well it compresses the state!\n",
        "\n",
        "To do this, we first apply our Autoencoder to a $5$ qubit Domain Wall state. After applying this state, the compressed state should be of the form $|00\\rangle$. Therefore resetting the last two qubits should not effect our over all state.\n",
        "\n",
        "After resetting we apply our decoder (the hermitian conjugate of our encoder) and compare it to the initial state by determining the fidelity. If our fidelity is one, then our Autoencoder has encoded all the information of the domain wall efficiently into a smaller set of qubits and when decoding, we retain the original state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c88b086"
      },
      "source": [
        "Let's first apply our circuit to the Domain Wall State, using the parameters we obtained when training our Quantum Autoencoder. (Note we have included barriers in our circuit below, however these are not necessary for the implementation of the Quantum Autoencoder and are used to determine between different sections of our circuit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oleHqLZ5slYj"
      },
      "outputs": [],
      "source": [
        "# Initialize a quantum circuit with (num_latent + num_trash) qubits\n",
        "test_qc = QuantumCircuit(num_latent + num_trash)\n",
        "\n",
        "# Compose the domain wall circuit onto the test circuit\n",
        "test_qc = test_qc.compose(domain_wall_circuit)\n",
        "\n",
        "# Generate the ansatz circuit with the same number of qubits\n",
        "ansatz_qc = ansatz(num_latent + num_trash)\n",
        "\n",
        "# Compose the ansatz circuit onto the test circuit\n",
        "test_qc = test_qc.compose(ansatz_qc)\n",
        "\n",
        "# Add a barrier for visual separation in the circuit diagram\n",
        "test_qc.barrier()\n",
        "\n",
        "# Reset qubits 4 and 3 to the |0> state (discard any prior state)\n",
        "test_qc.reset(4)\n",
        "test_qc.reset(3)\n",
        "\n",
        "# Add another barrier for clarity after resets\n",
        "test_qc.barrier()\n",
        "\n",
        "# Compose the inverse of the ansatz circuit onto the test circuit\n",
        "# This essentially attempts to reverse the ansatz operations applied earlier\n",
        "test_qc = test_qc.compose(ansatz_qc.inverse())\n",
        "\n",
        "# Draw the complete test circuit using matplotlib with the 'clifford' style for clear visualization\n",
        "test_qc.draw(output=\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grand-canal"
      },
      "source": [
        "Now we assign the parameter values obtained in the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mPnA-XQgsn3A"
      },
      "outputs": [],
      "source": [
        "# Assign the optimized parameter values (from the optimization result `opt_result.x`)\n",
        "# to the parameterized gates in the quantum circuit `test_qc`\n",
        "test_qc = test_qc.assign_parameters(opt_result.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dce4200"
      },
      "source": [
        "Now let's get the statevectors of our Domain Wall state and output circuit and calculate the fidelity!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diF_yPeVsqWo"
      },
      "outputs": [],
      "source": [
        "# Compute the statevector (quantum state) corresponding to the domain wall circuit\n",
        "domain_wall_state = Statevector(domain_wall_circuit).data\n",
        "\n",
        "# Compute the statevector corresponding to the test circuit after parameter assignment\n",
        "output_state = Statevector(test_qc).data\n",
        "\n",
        "# Calculate the fidelity between the domain wall input state and the output state\n",
        "# Fidelity measures the closeness or overlap between the two quantum states\n",
        "fidelity = np.sqrt(np.dot(domain_wall_state.conj(), output_state) ** 2)\n",
        "\n",
        "# Print the real part of the fidelity value, indicating how well the output state matches the input\n",
        "print(\"Fidelity of our Output State with our Input State: \", fidelity.real)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "618128d3"
      },
      "source": [
        "As you can see our fidelity is quite high and our Autoencoder has thus compressed our dataset while retaining all the information from the input state!\n",
        "\n",
        "Now we will see if we can apply such a Quantum Autoencoder to more complicated datasets containing noise, such as images of the numbers zero and one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6f37a6"
      },
      "source": [
        "### 2.9. A Quantum Autoencoder for Digital Compression\n",
        "\n",
        "One can also apply a Quantum Autoencoder to more complicated examples, such as a set of handwritten digits in order to compress the dataset. Below, we will show that we can indeed train an Quantum Autoencoder to compress such an example, giving us the ability to store data more efficiently on a Quantum Computer.\n",
        "\n",
        "For this tutorial, we will build a Quantum Autoencoder for a noisy dataset containing zeros and ones, which can be seen below.\n",
        "\n",
        "Each image contains $32$ pixels of which can be encoded into $5$ qubits by Amplitude Encoding. This can be done using Qiskit Machine Learning's `RawFeatureVector` feature map.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYgKMgKAsuex"
      },
      "outputs": [],
      "source": [
        "# Function to return pixel coordinates representing a '0' digit pattern\n",
        "def zero_idx(j, i):\n",
        "    # Returns a list of coordinates for pixels that should be bright to form digit zero\n",
        "    return [\n",
        "        [i, j],\n",
        "        [i - 1, j - 1],\n",
        "        [i - 1, j + 1],\n",
        "        [i - 2, j - 1],\n",
        "        [i - 2, j + 1],\n",
        "        [i - 3, j - 1],\n",
        "        [i - 3, j + 1],\n",
        "        [i - 4, j - 1],\n",
        "        [i - 4, j + 1],\n",
        "        [i - 5, j],\n",
        "    ]\n",
        "\n",
        "# Function to return pixel coordinates representing a '1' digit pattern\n",
        "def one_idx(i, j):\n",
        "    # Returns a list of coordinates for pixels that should be bright to form digit one\n",
        "    return [[i, j - 1], [i, j - 2], [i, j - 3], [i, j - 4], [i, j - 5], [i - 1, j - 4], [i, j]]\n",
        "\n",
        "# Function to generate a dataset of '0' and '1' digit images with noise, optionally drawing them\n",
        "def get_dataset_digits(num, draw=True):\n",
        "    train_images = []  # List to store generated images\n",
        "    train_labels = []  # Corresponding labels (0 or 1)\n",
        "\n",
        "    # Generate half of the dataset as '1's\n",
        "    for i in range(int(num / 2)):\n",
        "        # Create a noisy background by sampling values from uniform [0, 0.1)\n",
        "        empty = np.array([algorithm_globals.random.uniform(0, 0.1) for i in range(32)]).reshape(8, 4)\n",
        "\n",
        "        # Insert bright pixels for '1' digit at specified coordinates\n",
        "        for i, j in one_idx(2, 6):\n",
        "            empty[j][i] = algorithm_globals.random.uniform(0.9, 1)\n",
        "\n",
        "        train_images.append(empty)  # Add the image to the dataset\n",
        "        train_labels.append(1)      # Label it as '1'\n",
        "\n",
        "        # Optionally display the generated '1' image\n",
        "        if draw:\n",
        "            plt.title(\"This is a One\")\n",
        "            plt.imshow(train_images[-1])\n",
        "            plt.show()\n",
        "\n",
        "    # Generate half of the dataset as '0's\n",
        "    for i in range(int(num / 2)):\n",
        "        # Create noisy background as before\n",
        "        empty = np.array([algorithm_globals.random.uniform(0, 0.1) for i in range(32)]).reshape(8, 4)\n",
        "\n",
        "        # Insert bright pixels for '0' digit using zero_idx coordinates\n",
        "        for k, j in zero_idx(2, 6):\n",
        "            empty[k][j] = algorithm_globals.random.uniform(0.9, 1)\n",
        "\n",
        "        train_images.append(empty)  # Add the image to the dataset\n",
        "        train_labels.append(0)      # Label it as '0'\n",
        "\n",
        "        # Optionally display the generated '0' image\n",
        "        if draw:\n",
        "            plt.imshow(train_images[-1])\n",
        "            plt.title(\"This is a Zero\")\n",
        "            plt.show()\n",
        "\n",
        "    # Convert list of images to a NumPy array\n",
        "    train_images = np.array(train_images)\n",
        "\n",
        "    # Flatten images from 8x4 to 32-length vectors for model input\n",
        "    train_images = train_images.reshape(len(train_images), 32)\n",
        "\n",
        "    # Normalize each image vector to unit length (L2 norm = 1)\n",
        "    for i in range(len(train_images)):\n",
        "        sum_sq = np.sum(train_images[i] ** 2)\n",
        "        train_images[i] = train_images[i] / np.sqrt(sum_sq)\n",
        "\n",
        "    # Return normalized images and their corresponding labels\n",
        "    return train_images, train_labels\n",
        "\n",
        "# Generate a small sample dataset of 2 digit images without drawing them\n",
        "train_images, __ = get_dataset_digits(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646f12b6"
      },
      "source": [
        "After encoding our image into $5$ qubits, we begin to train our Quantum Autoencoder to compress this state into $3$ qubits.\n",
        "\n",
        "We repeat the steps in the previous example and write a cost function, again based on the Swap Test between the trash and latent space. We can also use the same Autoencoder function as given in the previous example, as the input state and trash space contain the same amount of qubits.\n",
        "\n",
        "Let's input one of our digits and see our circuit for the Autoencoder below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIOB81iqswhj"
      },
      "outputs": [],
      "source": [
        "# Set the number of latent and trash qubits for the autoencoder\n",
        "num_latent = 3\n",
        "num_trash = 2\n",
        "\n",
        "# Create a RawFeatureVector feature map for 2^(num_latent + num_trash) dimensions\n",
        "# This encodes classical data as quantum basis states over (num_latent + num_trash) qubits\n",
        "fm = RawFeatureVector(2 ** (num_latent + num_trash))\n",
        "\n",
        "# Generate the autoencoder quantum circuit with specified latent and trash qubits\n",
        "ae = auto_encoder_circuit(num_latent, num_trash)\n",
        "\n",
        "# Initialize a quantum circuit with total qubits for latent, trash, and auxiliary plus one classical bit\n",
        "qc = QuantumCircuit(num_latent + 2 * num_trash + 1, 1)\n",
        "\n",
        "# Compose the RawFeatureVector feature map on the first (num_latent + num_trash) qubits\n",
        "qc = qc.compose(fm, range(num_latent + num_trash))\n",
        "\n",
        "# Compose the autoencoder circuit on the entire circuit\n",
        "qc = qc.compose(ae)\n",
        "\n",
        "# Draw the complete circuit using matplotlib with the 'clifford' style for clear visualization\n",
        "qc.draw(output=\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "effd4db6"
      },
      "source": [
        "Again, we can see the swap test being performed on the qubits $3$, $4$, $5$ and $6$, which will determine the value of our cost function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM23WMC4szTW"
      },
      "outputs": [],
      "source": [
        "# Define an interpretation function that returns the measurement outcomes as is (identity function)\n",
        "def identity_interpret(x):\n",
        "    return x\n",
        "\n",
        "# Create a SamplerQNN (quantum neural network using sampling) with the following configuration:\n",
        "qnn = SamplerQNN(\n",
        "    circuit=qc,                 # The composed quantum circuit combining feature map and autoencoder\n",
        "    input_params=fm.parameters, # Parameters related to the RawFeatureVector feature map (input encoding)\n",
        "    weight_params=ae.parameters,# Trainable parameters of the autoencoder circuit (weights)\n",
        "    interpret=identity_interpret, # Use the identity interpretation function to keep raw measurement results\n",
        "    output_shape=2,             # Define the output shape, e.g., for binary outcomes\n",
        "    sampler=sampler,            # The sampler primitive to execute the quantum circuit and collect samples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inner-second"
      },
      "source": [
        "We build our cost function, based on the swap test between the reference and trash space for the digit dataset. To do this, we again use Qiskit Machine Learning's CircuitQNN network and use the same interpret function as we are measuring the probability of getting the final qubit in the $|1\\rangle$ state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ARzIr4aPs1QR"
      },
      "outputs": [],
      "source": [
        "# Define a cost function to evaluate the performance of the quantum neural network on digit classification\n",
        "def cost_func_digits(params_values):\n",
        "    # Perform a forward pass on the QNN with training images and current parameter values\n",
        "    probabilities = qnn.forward(train_images, params_values)\n",
        "\n",
        "    # Calculate the average probability of measuring '1' across all training samples\n",
        "    cost = np.sum(probabilities[:, 1]) / train_images.shape[0]\n",
        "\n",
        "    # Plotting the cost progression during optimization\n",
        "    clear_output(wait=True)  # Clear previous plot to update dynamically\n",
        "    objective_func_vals.append(cost)  # Append current cost value for plotting\n",
        "\n",
        "    # Set plot title and axis labels\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "\n",
        "    # Plot all recorded objective function values\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()\n",
        "\n",
        "    # Return the computed cost for optimization\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d868874b"
      },
      "source": [
        "Since model training may take a long time we have already pre-trained the model for some iterations and saved the pre-trained weights. We'll continue training from that point by setting `initial_point` to a vector of pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AQBbBasUs3Li"
      },
      "outputs": [],
      "source": [
        "# Open and read the JSON file containing precomputed initial parameter values for the quantum autoencoder\n",
        "with open(\"12_qae_initial_point.json\", \"r\") as f:\n",
        "    initial_point = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99a0c03"
      },
      "source": [
        "By minimizing this cost function, we can thus determine the required parameters to compress our noisy images. Let's see if we can encode our images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kURPCPDhs4r4"
      },
      "outputs": [],
      "source": [
        "# Initialize the COBYLA optimizer with a maximum of 150 iterations\n",
        "opt = COBYLA(maxiter=150)\n",
        "\n",
        "# Initialize an empty list to store the objective function values during optimization\n",
        "objective_func_vals = []\n",
        "\n",
        "# Set figure size for better visualization of the loss plot\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Record the starting time of the optimization\n",
        "start = time.time()\n",
        "\n",
        "# Run the optimizer to minimize the cost function starting from the given initial parameters\n",
        "opt_result = opt.minimize(fun=cost_func_digits, x0=initial_point)\n",
        "\n",
        "# Calculate elapsed time after optimization finishes\n",
        "elapsed = time.time() - start\n",
        "\n",
        "# Print the time taken to fit/train the model\n",
        "print(f\"Fit in {elapsed:0.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c92af0a"
      },
      "source": [
        "Looks like it has converged!\n",
        "\n",
        "Now let's build our Encoder and Decoder using the parameters obtained from the training period. After applying this circuit to our new dataset, we can then compare our input and output data and see if we were able to retain the images efficiently throughout the compression!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7EDk1kxs6pR"
      },
      "outputs": [],
      "source": [
        "# Testing phase: construct a test quantum circuit and evaluate the autoencoder on new images\n",
        "\n",
        "# Initialize a quantum circuit with qubits equal to latent + trash qubits\n",
        "test_qc = QuantumCircuit(num_latent + num_trash)\n",
        "\n",
        "# Compose the RawFeatureVector feature map (input encoding) onto the test circuit\n",
        "test_qc = test_qc.compose(fm)\n",
        "\n",
        "# Generate the ansatz circuit with latent + trash qubits\n",
        "ansatz_qc = ansatz(num_latent + num_trash)\n",
        "\n",
        "# Compose the ansatz circuit onto the test circuit\n",
        "test_qc = test_qc.compose(ansatz_qc)\n",
        "\n",
        "# Add a barrier for clarity\n",
        "test_qc.barrier()\n",
        "\n",
        "# Reset qubits 4 and 3 to |0⟩ to discard prior states (simulate trash qubits)\n",
        "test_qc.reset(4)\n",
        "test_qc.reset(3)\n",
        "\n",
        "# Add another barrier for visualization separation\n",
        "test_qc.barrier()\n",
        "\n",
        "# Compose the inverse of the ansatz to complete the autoencoder decoding step\n",
        "test_qc = test_qc.compose(ansatz_qc.inverse())\n",
        "\n",
        "# Generate new test images and labels (without displaying them)\n",
        "test_images, test_labels = get_dataset_digits(2, draw=False)\n",
        "\n",
        "# Loop over each test image and its label to analyze reconstruction\n",
        "for image, label in zip(test_images, test_labels):\n",
        "    # Assign the classical image data to the feature map parameters\n",
        "    original_qc = fm.assign_parameters(image)\n",
        "    # Compute the statevector of the original input image circuit\n",
        "    original_sv = Statevector(original_qc).data\n",
        "    # Reshape the probability amplitudes squared to 8x4 image format for visualization\n",
        "    original_sv = np.reshape(np.abs(original_sv) ** 2, (8, 4))\n",
        "\n",
        "    # Combine input image and optimized parameters for the test circuit\n",
        "    param_values = np.concatenate((image, opt_result.x))\n",
        "    # Assign these parameters to the test circuit\n",
        "    output_qc = test_qc.assign_parameters(param_values)\n",
        "    # Compute the statevector of the output circuit after encoding-decoding\n",
        "    output_sv = Statevector(output_qc).data\n",
        "    # Reshape output probabilities for visualization as image\n",
        "    output_sv = np.reshape(np.abs(output_sv) ** 2, (8, 4))\n",
        "\n",
        "    # Plot the original input image and the reconstructed output side-by-side\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    ax1.imshow(original_sv)\n",
        "    ax1.set_title(\"Input Data\")\n",
        "    ax2.imshow(output_sv)\n",
        "    ax2.set_title(\"Output Data\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ecfe78e"
      },
      "source": [
        "## Tasks 1.\n",
        "\n",
        "1. What is the main purpose of a quantum convolutional neural network?\n",
        "2. How does a QCNN differ from a classical CNN in terms of architecture?\n",
        "3. What role do convolutional layers play in QCNNs?\n",
        "4. How are pooling layers implemented in QCNNs?\n",
        "5. Why are parameterized quantum circuits important in QCNNs?\n",
        "6. What is the significance of the swap test in quantum autoencoders?\n",
        "7. How does entanglement contribute to the feature extraction in QCNNs?\n",
        "8. What types of quantum gates are typically used in QCNN convolution layers?\n",
        "9. How does measurement affect the training process in QCNNs?\n",
        "10. What challenges does quantum noise introduce in QCNN training and inference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAw55mgL2QvJ"
      },
      "source": [
        "## Answers to Tasks 1.\n",
        "\n",
        "### Write your answers here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRYmmA590Nks"
      },
      "source": [
        "## Tasks 2.\n",
        "\n",
        "1. Is `RealAmplitudes` an `ansatz`?\n",
        "2. How many reps are in `ansatz`?\n",
        "3. Does `auto_encoder_circuit` use swap test?\n",
        "4. Is auxiliary qubit measured?\n",
        "5. Does `domain_wall` apply X gates?\n",
        "6. Are `X` gates applied on half qubits?\n",
        "7. Does `pool_circuit` use `Rz` rotations?\n",
        "8. Are CNOT gates used in `pool_circuit`?\n",
        "9. Is pool_layer built from `pool_circuit`?\n",
        "10. Does `conv_layer` include barriers?\n",
        "11. Is `SparsePauliOp` used to define observables?\n",
        "12. Is `EstimatorQNN` initialized with decomposed circuit?\n",
        "13. Does `SamplerQNN` use sampling?\n",
        "14. How many latent qubits are used in the autoencoder circuit?\n",
        "15. Which register type holds classical bits?\n",
        "16. What method visualizes quantum circuits?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7llJur5z2du2"
      },
      "source": [
        "## Answers to Tasks 2.\n",
        "\n",
        "### Write your answers here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFAEdG6A0UzA"
      },
      "source": [
        "## Tasks 3.\n",
        "\n",
        "1. What is the primary goal of a quantum autoencoder?\n",
        "2. How does a quantum autoencoder compress quantum data?\n",
        "3. What is the role of trash qubits in a QAE?\n",
        "4. How does the swap test function in the training of a QAE?\n",
        "5. Why are variational circuits used in quantum autoencoders?\n",
        "6. How is the fidelity metric used to evaluate a QAE's performance?\n",
        "7. What advantages do quantum autoencoders have over classical autoencoders?\n",
        "8. How does measurement affect the reconstruction process in a QAE?\n",
        "9. What are the main sources of error during QAE training?\n",
        "10. How does parameter optimization work in quantum autoencoders?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlFfX21d2fID"
      },
      "source": [
        "## Answers to Tasks 3.\n",
        "\n",
        "### Write your answers here:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
